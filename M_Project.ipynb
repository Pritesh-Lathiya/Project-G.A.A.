{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests nbformat openai langchain"
      ],
      "metadata": {
        "id": "YwIqSV7eUf6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "GspF3TbpZfw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install GitPython"
      ],
      "metadata": {
        "id": "DnpLZHNhUf28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "yzn3p0PDVBxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overall Work :\n",
        "This code analyzes code repositories on GitHub to identify the most challenging repository based on technical complexity. It preprocesses the code, calculates complexity scores using code size and duplication impact, and generates a GPT analysis justifying the selection. The script also includes functions for tokenizing code, normalizing variable and function names, and finding duplicated code snippets. The OpenAI and GitHub API keys are utilized for authentication."
      ],
      "metadata": {
        "id": "crQC4SHwZQ69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib\n",
        "import requests\n",
        "import re\n",
        "from langchain.llms import openai\n",
        "import openai\n",
        "from urllib.parse import urlparse\n",
        "import nbformat\n",
        "from nbconvert import PythonExporter\n",
        "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
        "from io import BytesIO\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = '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'\n",
        "\n",
        "# Set your GitHub API key\n",
        "github_api_key = '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'\n",
        "\n",
        "MAX_TOKENS = 1000  # Set the maximum token limit for GPT\n",
        "\n",
        "\n",
        "def preprocess_code(repository):\n",
        "    if 'code' in repository:\n",
        "        code = repository['code']\n",
        "\n",
        "        # Remove comments and whitespace\n",
        "        code = re.sub(r'\\/\\/.*', '', code)  # Remove single-line comments\n",
        "        code = re.sub(r'\\/\\*(\\*(?!\\/)|[^*])*\\*\\/', '', code)  # Remove multi-line comments\n",
        "        code = code.strip()  # Remove leading/trailing whitespace\n",
        "        code = re.sub(r'\\s+', ' ', code)  # Collapse multiple consecutive spaces\n",
        "\n",
        "        # Normalize variable and function names\n",
        "        code = normalize_variable_names(code)\n",
        "        code = normalize_function_names(code)\n",
        "\n",
        "        # Extract code snippets from Jupyter notebooks\n",
        "        if code.startswith('%') or code.startswith('!'):\n",
        "            notebook = nbformat.reads(code, nbformat.NO_CONVERT)\n",
        "            python_exporter = PythonExporter()\n",
        "            (python_code, _) = python_exporter.from_notebook_node(notebook)\n",
        "            code = python_code.strip()\n",
        "\n",
        "        # Handle large file sizes (e.g., split into smaller chunks)\n",
        "        if len(code) > MAX_TOKENS:\n",
        "            # Split the code into smaller chunks to fit within the token limit\n",
        "            chunks = []\n",
        "            current_chunk = \"\"\n",
        "            for line in code.split('\\n'):\n",
        "                if len(current_chunk + line) < MAX_TOKENS:\n",
        "                    current_chunk += line + '\\n'\n",
        "                else:\n",
        "                    chunks.append(current_chunk.strip())\n",
        "                    current_chunk = line + '\\n'\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            code = chunks\n",
        "\n",
        "        # Convert code to a suitable format for analysis (e.g., AST, tokenization)\n",
        "        code = tokenize_code(code)\n",
        "\n",
        "        # Apply preprocessing steps here\n",
        "        preprocessed_code = code.strip()\n",
        "\n",
        "        # Update the repository object or return the preprocessed code\n",
        "        repository['code'] = preprocessed_code\n",
        "        # Or return the preprocessed code directly: return preprocessed_code\n",
        "\n",
        "    return repository\n",
        "\n",
        "\n",
        "def normalize_variable_names(code):\n",
        "    code = re.sub(r'\\bvar\\b', 'normalized_var', code)\n",
        "    return code\n",
        "\n",
        "\n",
        "def normalize_function_names(code):\n",
        "    code = re.sub(r'\\bfunc\\b', 'normalized_func', code)\n",
        "    return code\n",
        "\n",
        "\n",
        "def tokenize_code(code):\n",
        "    # Tokenize the code using Python's tokenize module\n",
        "    tokens = tokenize(BytesIO(code.encode('utf-8')).readline)\n",
        "\n",
        "    # Filter out tokens that are not relevant for analysis\n",
        "    filtered_tokens = [\n",
        "        token for token in tokens\n",
        "        if token.type in (NUMBER, STRING, NAME, OP)\n",
        "    ]\n",
        "\n",
        "    # Untokenize the filtered tokens to obtain the transformed code\n",
        "    transformed_code = untokenize(filtered_tokens).decode('utf-8')\n",
        "\n",
        "    return transformed_code\n",
        "\n",
        "\n",
        "def assess_repository(repository):\n",
        "    # Implement prompt engineering when passing code through GPT for evaluation to determine its technical complexity\n",
        "\n",
        "    # Extract the name and description from the repository\n",
        "    name = repository['name'] or \"\"\n",
        "    description = repository['description'] or \"\"\n",
        "\n",
        "    # Generate a textual description/summary of the repository using GPT\n",
        "    response = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=name + \"\\n\" + description,\n",
        "        max_tokens=100\n",
        "    )\n",
        "    summary = response.choices[0].text.strip()\n",
        "\n",
        "    # Assess the technical complexity of the repository based on the generated summary\n",
        "    complexity_score = len(summary.split(' '))  # Placeholder value\n",
        "\n",
        "    return complexity_score\n",
        "\n",
        "\n",
        "def analyze_repository(repository):\n",
        "    if 'code' in repository:\n",
        "        code = repository['code']\n",
        "\n",
        "        # Calculate code size\n",
        "        lines_of_code = len(code.split('\\n'))\n",
        "\n",
        "        # Calculate code duplication impact\n",
        "        duplication_impact = calculate_duplication_impact(code)\n",
        "\n",
        "        # Calculate complexity score based on code size and duplication impact\n",
        "        complexity_score = lines_of_code + duplication_impact\n",
        "\n",
        "        return complexity_score\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def calculate_duplication_impact(code):\n",
        "    # Identify duplicated code snippets and measure their impact\n",
        "    duplicated_code = find_duplicated_code(code)\n",
        "\n",
        "    # Calculate the impact of code duplication\n",
        "    duplication_impact = len(duplicated_code) * 0.5  # Placeholder value\n",
        "\n",
        "    return duplication_impact\n",
        "\n",
        "\n",
        "def find_duplicated_code(code):\n",
        "    # Example using difflib to find similar lines of code\n",
        "    lines = code.split('\\n')\n",
        "    duplicated_code = []\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        for j in range(i + 1, len(lines)):\n",
        "            similarity = difflib.SequenceMatcher(None, line, lines[j]).ratio()\n",
        "            if similarity > 0.8:  # Adjust the similarity threshold as needed\n",
        "                duplicated_code.append(line)\n",
        "\n",
        "    return duplicated_code\n",
        "\n",
        "\n",
        "def get_most_challenging_repository(profile_url):\n",
        "    # Extract the username from the GitHub profile URL\n",
        "    parsed_url = urlparse(profile_url)\n",
        "    if parsed_url.netloc == 'github.com':\n",
        "        path_parts = parsed_url.path.split('/')\n",
        "        if len(path_parts) >= 2:\n",
        "            username = path_parts[1]\n",
        "        else:\n",
        "            print(\"Invalid GitHub profile URL.\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"Invalid GitHub profile URL.\")\n",
        "        return None\n",
        "\n",
        "    # Fetch user repositories from GitHub API\n",
        "    headers = {\n",
        "        'Authorization': f'token {github_api_key}'\n",
        "    }\n",
        "    response = requests.get(f'https://api.github.com/users/{username}/repos', headers=headers)\n",
        "    repositories = response.json()\n",
        "\n",
        "    most_challenging_repository = None\n",
        "    highest_complexity_score = 0\n",
        "\n",
        "    for repository in repositories:\n",
        "        repository = preprocess_code(repository)\n",
        "        complexity_score = analyze_repository(repository)\n",
        "        complexity_challenge_score = assess_repository(repository)\n",
        "\n",
        "        if complexity_challenge_score > highest_complexity_score:\n",
        "            highest_complexity_score = complexity_challenge_score\n",
        "            most_challenging_repository = repository\n",
        "\n",
        "    return most_challenging_repository\n",
        "\n",
        "\n",
        "# Example usage\n",
        "github_profile_url = 'https://github.com/Pritesh-Lathiya'\n",
        "most_challenging_repo = get_most_challenging_repository(github_profile_url)\n",
        "\n",
        "if most_challenging_repo:\n",
        "    # Generate a GPT analysis justifying the selection\n",
        "    response = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=f\"I selected [{most_challenging_repo['name']}]({most_challenging_repo['html_url']}) as the most complex repository because...\",\n",
        "        max_tokens=100\n",
        "    )\n",
        "    gpt_analysis = response.choices[0].text.strip()\n",
        "    print(\"Most complex repository:\", most_challenging_repo['name'])\n",
        "    print(\"Most complex repository:\", most_challenging_repo['html_url'])\n",
        "    print(\"GPT Analysis:\", gpt_analysis)\n",
        "else:\n",
        "    print(\"No repositories found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuxGM_gqXG6x",
        "outputId": "d5592a6b-e24a-4deb-911a-f852fdd9e76c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most complex  repository: Simple-Linear-Regression\n",
            "Most complex  repository: https://github.com/Pritesh-Lathiya/Simple-Linear-Regression\n",
            "GPT Analysis: Simple-Linear-Regression is a robust machine learning library that enables users to implement the core concepts of linear regression. It has a comprehensive set of features, such as a wide range of linear regression algorithms, support for parallel computation, and the ability to use multiple sources of data. The complexity of the repository comes from its use of advanced algorithms and the vast amount of parameters and settings users can adjust in order to produce the most accurate results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Work Of Code**\n",
        "\n",
        "* Importing Required Libraries: The script begins by importing the necessary libraries for the code analysis, including difflib, requests, re, urlparse from urllib.parse, nbformat, PythonExporter from nbconvert, tokenize and untokenize from tokenize, and BytesIO from io.\n",
        "\n",
        "1. Setting API Keys: The script sets the OpenAI API key and the GitHub API key in variables named openai.api_key and github_api_key, respectively. These keys are used to authenticate and access the respective APIs.\n",
        "\n",
        "2. Setting Maximum Tokens: The script defines a constant variable MAX_TOKENS that represents the maximum number of tokens allowed for the GPT model. It is set to 1000 tokens.\n",
        "\n",
        "3. Preprocessing Code: The preprocess_code function takes a repository object as input and performs preprocessing steps on the code within the repository. It removes comments and whitespace, normalizes variable and function names, extracts code snippets from Jupyter notebooks, and handles large file sizes by splitting the code into smaller chunks if needed. The code is then tokenized using Python's tokenize module and converted to a suitable format for analysis. The preprocessed code is returned or updated in the repository object.\n",
        "\n",
        "4. Normalizing Variable and Function Names: The normalize_variable_names and normalize_function_names functions are helper functions used in the preprocessing step to replace variable and function names with normalized names. In the provided code, they replace occurrences of the keywords \"var\" and \"func\" with \"normalized_var\" and \"normalized_func\", respectively.\n",
        "\n",
        "5. Tokenizing Code: The tokenize_code function tokenizes the code using Python's tokenize module. It filters out tokens that are not relevant for analysis (such as indentation) and untokenizes the filtered tokens to obtain the transformed code.\n",
        "\n",
        "6. Assessing Repository Complexity: The assess_repository function takes a repository object as input and generates a textual description/summary of the repository using the OpenAI GPT (text-davinci-003) model. It calculates the complexity score based on the generated summary, which is currently a placeholder value representing the number of words in the summary.\n",
        "\n",
        "7. Analyzing Repository: The analyze_repository function takes a repository object as input and calculates the complexity score based on the code size (number of lines of code) and the duplication impact. The duplication impact is calculated using the calculate_duplication_impact function.\n",
        "\n",
        "8. Calculating Duplication Impact: The calculate_duplication_impact function identifies duplicated code snippets in the code and measures their impact. It uses the find_duplicated_code function, which compares each line of code with all subsequent lines to find similar lines based on a similarity threshold. The impact of code duplication is currently a placeholder value calculated as half the number of duplicated lines.\n",
        "\n",
        "9. Finding Duplicated Code: The find_duplicated_code function uses the difflib library to find similar lines of code. It iterates over each line of code and compares it with all subsequent lines to find similarities based on a similarity threshold. The similar lines are stored in the duplicated_code list.\n",
        "\n",
        "10. Getting Most Challenging Repository: The get_most_challenging_repository function takes a GitHub profile URL as input, extracts the username from the URL, and fetches the user's repositories using the GitHub API. It iterates over the repositories, preprocesses the code, calculates the complexity score"
      ],
      "metadata": {
        "id": "J54redA4Y4bI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xfNPze5fZF3j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}